phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.45e+08
ite:5000,mse:2.56e+04
ite:10000,mse:1.49e+03
ite:15000,mse:8.42e+02
ite:20000,mse:8.73e+02
ite:25000,mse:6.08e+02
ite:30000,mse:5.50e+02
ite:35000,mse:5.17e+02
ite:40000,mse:5.17e+02
ite:45000,mse:4.29e+02
ite:50000,mse:4.47e+02
ite:55000,mse:9.02e+02
ite:60000,mse:4.07e+02
ite:65000,mse:3.95e+02
ite:70000,mse:3.78e+02
ite:75000,mse:3.99e+02
ite:80000,mse:3.68e+02
ite:85000,mse:3.67e+02
ite:90000,mse:3.95e+02
ite:95000,mse:3.82e+02
time: 4.29e+01ite/s
training mse: 3.66e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:7.36e+06
ite:5000,mse:2.84e+03
ite:10000,mse:1.13e+03
ite:15000,mse:6.75e+02
ite:20000,mse:4.10e+02
ite:25000,mse:4.66e+02
ite:30000,mse:3.27e+02
ite:35000,mse:3.36e+02
ite:40000,mse:3.37e+02
ite:45000,mse:2.87e+02
ite:50000,mse:3.31e+02
ite:55000,mse:2.99e+02
ite:60000,mse:3.01e+02
ite:65000,mse:2.99e+02
ite:70000,mse:2.86e+02
ite:75000,mse:3.82e+02
ite:80000,mse:2.86e+02
ite:85000,mse:2.87e+02
ite:90000,mse:3.11e+02
ite:95000,mse:3.10e+02
time: 4.34e+01ite/s
training mse: 2.95e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:2.85e+06
ite:5000,mse:1.45e+03
ite:10000,mse:9.93e+02
ite:15000,mse:7.52e+02
ite:20000,mse:6.00e+02
ite:25000,mse:5.06e+02
ite:30000,mse:4.41e+02
ite:35000,mse:8.30e+02
ite:40000,mse:3.62e+02
ite:45000,mse:3.04e+02
ite:50000,mse:3.17e+02
ite:55000,mse:2.96e+02
ite:60000,mse:2.93e+02
ite:65000,mse:2.77e+02
ite:70000,mse:2.71e+02
ite:75000,mse:2.80e+02
ite:80000,mse:2.74e+02
ite:85000,mse:2.74e+02
ite:90000,mse:2.99e+02
ite:95000,mse:2.94e+02
time: 4.34e+01ite/s
training mse: 2.85e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:8.03e+06
ite:5000,mse:3.08e+03
ite:10000,mse:1.21e+03
ite:15000,mse:6.69e+02
ite:20000,mse:4.56e+02
ite:25000,mse:4.07e+02
ite:30000,mse:3.78e+02
ite:35000,mse:3.62e+02
ite:40000,mse:3.51e+02
ite:45000,mse:3.07e+02
ite:50000,mse:3.39e+02
ite:55000,mse:3.25e+02
ite:60000,mse:3.30e+02
ite:65000,mse:3.16e+02
ite:70000,mse:3.05e+02
ite:75000,mse:3.21e+02
ite:80000,mse:3.06e+02
ite:85000,mse:3.08e+02
ite:90000,mse:3.36e+02
ite:95000,mse:3.39e+02
time: 4.08e+01ite/s
training mse: 3.15e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.81e+05
ite:5000,mse:5.38e+02
ite:10000,mse:4.35e+02
ite:15000,mse:4.13e+02
ite:20000,mse:3.49e+02
ite:25000,mse:4.05e+02
ite:30000,mse:4.19e+02
ite:35000,mse:3.33e+02
ite:40000,mse:4.30e+02
ite:45000,mse:3.08e+02
ite:50000,mse:3.66e+02
ite:55000,mse:3.00e+02
ite:60000,mse:4.48e+02
ite:65000,mse:3.07e+02
ite:70000,mse:3.04e+02
ite:75000,mse:3.36e+02
ite:80000,mse:3.74e+02
ite:85000,mse:2.94e+02
ite:90000,mse:3.26e+02
ite:95000,mse:3.12e+02
time: 2.53e+01ite/s
training mse: 3.47e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.80e+05
ite:5000,mse:5.81e+02
ite:10000,mse:5.07e+02
ite:15000,mse:3.73e+02
ite:20000,mse:3.62e+02
ite:25000,mse:3.35e+02
ite:30000,mse:5.33e+02
ite:35000,mse:3.92e+02
ite:40000,mse:5.28e+02
ite:45000,mse:3.13e+02
ite:50000,mse:9.09e+02
ite:55000,mse:3.05e+02
ite:60000,mse:3.33e+02
ite:65000,mse:3.02e+02
ite:70000,mse:3.10e+02
ite:75000,mse:4.04e+02
ite:80000,mse:6.93e+02
ite:85000,mse:3.43e+02
ite:90000,mse:3.33e+02
ite:95000,mse:3.70e+02
time: 1.31e+01ite/s
training mse: 3.71e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.86e+05
ite:5000,mse:5.17e+02
ite:10000,mse:4.00e+02
ite:15000,mse:4.40e+02
ite:20000,mse:3.60e+02
ite:25000,mse:4.04e+02
ite:30000,mse:3.43e+02
ite:35000,mse:3.52e+02
ite:40000,mse:4.09e+02
ite:45000,mse:3.30e+02
ite:50000,mse:5.36e+02
ite:55000,mse:4.33e+02
ite:60000,mse:3.15e+02
ite:65000,mse:3.24e+02
ite:70000,mse:2.98e+02
ite:75000,mse:3.07e+02
ite:80000,mse:3.01e+02
ite:85000,mse:3.85e+02
ite:90000,mse:4.30e+02
ite:95000,mse:3.80e+02
time: 9.14e+00ite/s
training mse: 3.99e+02
2024-12-09 03:15:18.933771: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 21.34GiB (22918254784 bytes) by rematerialization; only reduced to 32.49GiB (34887000128 bytes), down from 32.49GiB (34887000128 bytes) originally
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:6.89e+05
ite:5000,mse:6.68e+02
ite:10000,mse:6.86e+02
ite:15000,mse:4.38e+02
ite:20000,mse:7.83e+02
ite:25000,mse:5.11e+02
ite:30000,mse:3.40e+02
ite:35000,mse:3.43e+02
ite:40000,mse:3.46e+02
ite:45000,mse:3.56e+02
ite:50000,mse:3.97e+02
ite:55000,mse:4.25e+02
ite:60000,mse:3.69e+02
ite:65000,mse:2.97e+02
ite:70000,mse:2.91e+02
ite:75000,mse:1.51e+03
ite:80000,mse:3.51e+02
ite:85000,mse:2.91e+02
ite:90000,mse:3.15e+02
ite:95000,mse:3.66e+02
time: 6.73e+00ite/s
training mse: 3.63e+02
2024-12-09 07:23:49.455156: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 12.58GiB (13513254784 bytes) by rematerialization; only reduced to 40.59GiB (43587000128 bytes), down from 40.59GiB (43587000128 bytes) originally
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:2.41e+05
ite:5000,mse:8.12e+02
ite:10000,mse:8.31e+02
ite:15000,mse:4.29e+02
ite:20000,mse:4.49e+02
ite:25000,mse:5.81e+02
ite:30000,mse:3.48e+02
ite:35000,mse:3.68e+02
ite:40000,mse:3.42e+02
ite:45000,mse:2.99e+02
ite:50000,mse:3.41e+02
ite:55000,mse:3.16e+02
ite:60000,mse:3.11e+02
ite:65000,mse:3.03e+02
ite:70000,mse:2.98e+02
ite:75000,mse:3.02e+02
ite:80000,mse:2.95e+02
ite:85000,mse:5.85e+02
ite:90000,mse:3.22e+02
ite:95000,mse:3.49e+02
time: 5.26e+00ite/s
training mse: 4.87e+02
2024-12-09 12:41:13.903126: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 3.83GiB (4108254784 bytes) by rematerialization; only reduced to 48.70GiB (52287000128 bytes), down from 48.70GiB (52287000128 bytes) originally
2024-12-09 12:41:23.950832: W external/xla/xla/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.72GiB (rounded to 7212000000)requested by op 
2024-12-09 12:41:23.950981: W external/xla/xla/tsl/framework/bfc_allocator.cc:494] ******************______*********************************************************************_______
E1209 12:41:23.951002 2802696 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 7212000000 bytes.
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
Traceback (most recent call last):
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 161, in <module>
    train(key)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 119, in train
    loss, model, opt_state = make_step(model, input_points, frozen_para, optim, opt_state)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 61, in make_step
    loss, grads = compute_loss_and_grads(model, ob_txy, frozen_para, gamma=args.gamma, eps=args.eps)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/anaconda3/lib/python3.12/site-packages/equinox/_ad.py", line 79, in __call__
    return fun_value_and_grad(diff_x, nondiff_x, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 54, in compute_loss
    output = vmap(net, (None, 0, None))(model, ob_txy[:, 0], frozen_para)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 50, in net
    return model(jnp.stack([x]), frozen_para)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/networks.py", line 181, in __call__
    x = self.layers[i](x, frozen_para[i])
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/networks.py", line 332, in __call__
    x_interp = jnp.sinc(x)
               ^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 7212000000 bytes.
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:2.78e+05
ite:5000,mse:3.48e+02
ite:10000,mse:3.09e+02
ite:15000,mse:2.89e+02
ite:20000,mse:2.96e+02
ite:25000,mse:2.73e+02
ite:30000,mse:3.67e+02
ite:35000,mse:2.85e+02
ite:40000,mse:3.01e+02
ite:45000,mse:2.50e+02
ite:50000,mse:2.81e+02
ite:55000,mse:2.66e+02
ite:60000,mse:2.73e+02
ite:65000,mse:2.98e+02
ite:70000,mse:3.93e+02
ite:75000,mse:2.75e+02
ite:80000,mse:2.84e+02
ite:85000,mse:2.62e+02
ite:90000,mse:2.89e+02
ite:95000,mse:2.86e+02
time: 9.30e+00ite/s
training mse: 2.71e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.99e+07
ite:5000,mse:3.73e+02
ite:10000,mse:3.10e+02
ite:15000,mse:1.01e+03
ite:20000,mse:7.13e+02
ite:25000,mse:3.85e+02
ite:30000,mse:2.83e+02
ite:35000,mse:5.09e+02
ite:40000,mse:2.91e+02
ite:45000,mse:2.59e+02
ite:50000,mse:2.88e+02
ite:55000,mse:2.73e+02
ite:60000,mse:3.01e+02
ite:65000,mse:4.69e+02
ite:70000,mse:3.36e+02
ite:75000,mse:7.89e+02
ite:80000,mse:2.76e+02
ite:85000,mse:2.68e+02
ite:90000,mse:6.54e+02
ite:95000,mse:6.62e+02
time: 1.28e+01ite/s
training mse: 2.88e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.45e+06
ite:5000,mse:3.11e+02
ite:10000,mse:3.16e+02
ite:15000,mse:1.08e+03
ite:20000,mse:3.19e+02
ite:25000,mse:2.91e+02
ite:30000,mse:2.95e+02
ite:35000,mse:3.11e+02
ite:40000,mse:3.05e+02
ite:45000,mse:2.62e+02
ite:50000,mse:7.35e+02
ite:55000,mse:2.72e+02
ite:60000,mse:7.23e+02
ite:65000,mse:2.83e+02
ite:70000,mse:2.65e+02
ite:75000,mse:3.31e+02
ite:80000,mse:2.67e+02
ite:85000,mse:2.87e+02
ite:90000,mse:3.39e+02
ite:95000,mse:4.09e+02
time: 8.57e+00ite/s
training mse: 2.87e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:1.05e+07
ite:5000,mse:1.72e+03
ite:10000,mse:3.17e+02
ite:15000,mse:4.94e+02
ite:20000,mse:3.55e+02
ite:25000,mse:3.13e+02
ite:30000,mse:2.92e+02
ite:35000,mse:7.61e+02
ite:40000,mse:3.16e+02
ite:45000,mse:6.31e+02
ite:50000,mse:3.44e+02
ite:55000,mse:4.25e+02
ite:60000,mse:5.27e+02
ite:65000,mse:2.85e+02
ite:70000,mse:2.58e+02
ite:75000,mse:2.82e+02
ite:80000,mse:2.72e+02
ite:85000,mse:2.76e+02
ite:90000,mse:2.94e+02
ite:95000,mse:3.88e+02
time: 5.14e+00ite/s
training mse: 2.82e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:6.38e+05
ite:5000,mse:1.05e+03
ite:10000,mse:9.99e+02
ite:15000,mse:5.21e+02
ite:20000,mse:4.49e+02
ite:25000,mse:7.99e+02
ite:30000,mse:3.56e+02
ite:35000,mse:6.18e+02
ite:40000,mse:8.32e+02
ite:45000,mse:3.32e+02
ite:50000,mse:2.93e+02
ite:55000,mse:3.11e+02
ite:60000,mse:4.62e+02
ite:65000,mse:2.78e+02
ite:70000,mse:3.47e+02
ite:75000,mse:2.90e+02
ite:80000,mse:3.41e+02
ite:85000,mse:4.05e+02
ite:90000,mse:3.75e+02
ite:95000,mse:6.34e+02
time: 3.73e+00ite/s
training mse: 2.80e+02
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
ite:0,mse:3.46e+05
ite:5000,mse:4.79e+02
ite:10000,mse:3.46e+02
ite:15000,mse:3.56e+02
ite:20000,mse:3.15e+02
ite:25000,mse:2.20e+03
ite:30000,mse:3.18e+02
ite:35000,mse:3.28e+02
ite:40000,mse:8.15e+02
ite:45000,mse:4.97e+02
ite:50000,mse:4.88e+02
ite:55000,mse:2.94e+02
ite:60000,mse:9.31e+02
ite:65000,mse:3.01e+02
ite:70000,mse:4.36e+02
ite:75000,mse:3.42e+02
ite:80000,mse:3.12e+02
ite:85000,mse:2.73e+02
ite:90000,mse:3.10e+02
ite:95000,mse:3.33e+02
time: 2.49e+00ite/s
training mse: 3.83e+02
2024-12-10 21:09:54.369873: W external/xla/xla/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.30GiB (rounded to 1400000000)requested by op 
2024-12-10 21:09:54.370365: W external/xla/xla/tsl/framework/bfc_allocator.cc:494] ****************************************************************************************************
E1210 21:09:54.370383 2987666 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1400000000 bytes.
phiN flattened shape: (19595264, 1)
dt_phiN flattened shape: (19595264, 1)
dxx_phiN flattened shape: (19595264, 1)
dyy_phiN flattened shape: (19595264, 1)
Traceback (most recent call last):
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 161, in <module>
    train(key)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 119, in train
    loss, model, opt_state = make_step(model, input_points, frozen_para, optim, opt_state)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 61, in make_step
    loss, grads = compute_loss_and_grads(model, ob_txy, frozen_para, gamma=args.gamma, eps=args.eps)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 54, in compute_loss
    output = vmap(net, (None, 0, None))(model, ob_txy[:, 0], frozen_para)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 50, in net
    return model(jnp.stack([x]), frozen_para)[0]
  File "/home/qjw/code/python_code/phase_field/networks.py", line 106, in __call__
    f = f * u + (1 - f) * v
  File "/home/qjw/anaconda3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py", line 747, in op
    return getattr(self.aval, f"_{name}")(self, *args)
  File "/home/qjw/anaconda3/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py", line 272, in deferring_binary_op
    return binary_op(*args)
jax._src.source_info_util.JaxStackTraceBeforeTransformation: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1400000000 bytes.

The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 161, in <module>
    train(key)
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 119, in train
    loss, model, opt_state = make_step(model, input_points, frozen_para, optim, opt_state)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/code/python_code/phase_field/free_energy_1.py", line 61, in make_step
    loss, grads = compute_loss_and_grads(model, ob_txy, frozen_para, gamma=args.gamma, eps=args.eps)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/qjw/anaconda3/lib/python3.12/site-packages/equinox/_ad.py", line 79, in __call__
    return fun_value_and_grad(diff_x, nondiff_x, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1400000000 bytes.
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
